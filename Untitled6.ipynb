{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPavlOmoYAW51FrBbEqgWtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weatherman85/gov_law_clf/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reb0_2FBe2xH",
        "outputId": "3be210a3-6fa8-4e17-be0a-926b08a66e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: TorchCRF in /usr/local/lib/python3.9/dist-packages (1.1.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch TorchCRF seqeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bMNzNyebfBbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uICoTKvf2xr",
        "outputId": "0064b535-102c-4e2d-9851-64361dfff7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-21 01:36:41.715336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-21 01:36:44.891367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 01:36:44.893614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 01:36:44.893651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (63.4.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "from typing import List\n",
        "from spacy.training import offsets_to_biluo_tags, biluo_to_iob\n",
        "nlp = spacy.blank(\"en\")\n",
        "LABEL = \"gov_law\"\n",
        "\n",
        "def format_doccano(data: List[dict]) -> List[dict]:\n",
        "    formatted_data = []\n",
        "    for d in data:\n",
        "        for item in d:\n",
        "            sentences = []\n",
        "            for p in item['paragraphs']:\n",
        "                text = p['raw']\n",
        "                sentence = {'text': text, 'entities': []}\n",
        "                for ent in p['entities']:\n",
        "                    start, end, label = ent\n",
        "                    sentence['entities'].append((start,end,label))\n",
        "                sentences.append(sentence)\n",
        "                formatted_data.append({'id': item['id'], 'paragraphs': sentences})\n",
        "    return formatted_data\n",
        "\n",
        "def doccano_to_iob(doccano_data: List[dict]) -> List[tuple]:\n",
        "    iob_data = []\n",
        "    for d in doccano_data:\n",
        "        for d in d['paragraphs']:\n",
        "            text = d['text']\n",
        "            entities = d['entities']\n",
        "            doc = nlp(text)\n",
        "            ents = offsets_to_biluo_tags(doc,entities)\n",
        "            ents = biluo_to_iob(ents)\n",
        "            iob_data.append((text,ents))\n",
        "    return iob_data\n",
        "\n",
        "# Load the Doccano JSONL file and format the data\n",
        "with open('./annotation_iob.json', 'r', encoding='utf-8') as f:\n",
        "    doccano_data = [json.loads(line) for line in f]\n",
        "\n",
        "formatted_data = format_doccano(doccano_data)\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "# Convert the formatted data to IOB format and save it as a spacy DocBin\n",
        "iob_data = doccano_to_iob(formatted_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwqAi4xTfW3T",
        "outputId": "d95957a5-3b97-45f9-f197-2913566bf490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by the internal l...\" with entities \"[(99, 128, 'gov_law'), (183, 214, 'gov_law'), (296...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement and any disputes relating to or ari...\" with entities \"[(157, 198, 'gov_law'), (404, 454, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The parties hereto irrevocably and unconditionally...\" with entities \"[(131, 170, 'gov_law'), (635, 654, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Note shall be governed by and construed in ac...\" with entities \"[(68, 97, 'gov_law'), (290, 309, 'gov_law'), (328,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by, and construed...\" with entities \"[(91, 121, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The governing law of this Second Amendment and all...\" with entities \"[(131, 159, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by the laws of th...\" with entities \"[(40, 71, 'gov_law'), (394, 433, 'gov_law'), (450,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement will be governed by and construed a...\" with entities \"[(84, 114, 'gov_law'), (301, 330, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This agreement shall be governed by and construed ...\" with entities \"[(73, 98, 'gov_law'), (245, 273, 'gov_law'), (304,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(72, 102, 'gov_law'), (295, 320, 'gov_law'), (338...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement and the legal relations among the p...\" with entities \"[(131, 159, 'gov_law'), (411, 424, 'gov_law'), (51...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Note shall be governed by and construed in ac...\" with entities \"[(69, 103, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement will be governed by the laws of the...\" with entities \"[(40, 72, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Note shall be governed by and construed in ac...\" with entities \"[(69, 95, 'gov_law'), (289, 312, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(82, 112, 'gov_law'), (436, 467, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement and all matters arising out of or r...\" with entities \"[(199, 229, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Any action taken to enforce this Agreement shall b...\" with entities \"[(80, 113, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement IS A contract UNDER the laws of the...\" with entities \"[(38, 68, 'gov_law'), (148, 178, 'gov_law'), (402,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be construed in accordance wi...\" with entities \"[(56, 83, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"NOTWITHSTANDING THE PLACE WHERE THIS AGREEMENT MAY...\" with entities \"[(190, 219, 'gov_law'), (429, 452, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed and construed in ...\" with entities \"[(70, 101, 'gov_law'), (216, 239, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"All matters or issues relating to the interpretati...\" with entities \"[(130, 160, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The provisions of this Agreement shall be construe...\" with entities \"[(74, 104, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Third Amendment shall be construed and govern...\" with entities \"[(59, 89, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"THIS AMENDMENT SHALL BE GOVERNED BY, AND CONSTRUED...\" with entities \"[(61, 91, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Company hereby submits to the exclusive jurisd...\" with entities \"[(87, 130, 'gov_law'), (142, 163, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The validity and effect of this Agreement shall be...\" with entities \"[(110, 138, 'gov_law'), (538, 573, 'gov_law'), (67...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(72, 102, 'gov_law'), (295, 319, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(133, 166, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"You and CBS agree that any disputes arising out of...\" with entities \"[(333, 365, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(73, 100, 'gov_law'), (298, 317, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Plan will be governed and construed in accorda...\" with entities \"[(62, 93, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The validity, interpretation, construction and per...\" with entities \"[(123, 152, 'gov_law'), (306, 356, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Any dispute regarding the reasonableness of the co...\" with entities \"[(297, 327, 'gov_law'), (427, 461, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"THIS AGREEMENT SHALL BE GOVERNED BY AND CONSTRUED ...\" with entities \"[(82, 111, 'gov_law'), (801, 839, 'gov_law'), (846...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Except to the extent preempted by federal law, the...\" with entities \"[(108, 138, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The Plan shall be governed by, and construed in ac...\" with entities \"[(68, 98, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by the laws of th...\" with entities \"[(39, 68, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"THE DOMESTIC LAW, WITHOUT REGARD TO CONFLICTS OF L...\" with entities \"[(48, 90, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement will be governed by and construed i...\" with entities \"[(71, 98, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement and the performance of the Parties ...\" with entities \"[(111, 156, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by and construed ...\" with entities \"[(73, 102, 'gov_law'), (284, 326, 'gov_law'), (334...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"(a) Each party hereto irrevocably and unconditiona...\" with entities \"[(410, 441, 'gov_law'), (503, 546, 'gov_law'), (81...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The rights and obligations of the parties shall be...\" with entities \"[(138, 180, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This Agreement shall be governed by the laws of th...\" with entities \"[(295, 344, 'gov_law'), (40, 65, 'gov_law')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(iob_data,columns=[\"text\",\"tag\"])"
      ],
      "metadata": {
        "id": "or0MonM1fZaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['labels'] = 1"
      ],
      "metadata": {
        "id": "sxF6dsBigBAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer,BertModel\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from TorchCRF import CRF\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "model_name = 'nlpaueb/bert-base-uncased-contracts'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert = BertModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs3NSkCOgCXS",
        "outputId": "afae720d-c5e7-4af2-bbd1-bc81382878a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/bert-base-uncased-contracts were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, texts, labels, ner_labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.ner_labels = ner_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        ner_label = self.ner_labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'classification_labels': torch.tensor(label, dtype=torch.long),\n",
        "            'ner_labels': torch.tensor(ner_label, dtype=torch.long)\n",
        "        }\n",
        "        \n",
        "def pad(l, content, width):\n",
        "    l.extend([content] * (width - len(l)))\n",
        "    return l "
      ],
      "metadata": {
        "id": "6KWUQDmqgFhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2idx = {\"B-gov_law\":0,\"I-gov_law\":1,\"O\":2,\"-\":2}"
      ],
      "metadata": {
        "id": "i932y-fPgGpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ner_labels'] = df['tag'].apply(lambda x: [tag2idx[y] for y in x])\n",
        "df['ner_labels'] = df['ner_labels'].apply(lambda x: pad(x,0,512))\n",
        "df['ner_labels'] = df['ner_labels'].apply(lambda x: x[:512])"
      ],
      "metadata": {
        "id": "axFT4IXOgHxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_len'] = df['ner_labels'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "zYH0auHem1DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['label_len']!=512]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "hHCVXhgOm5bv",
        "outputId": "163c6c04-bb70-48c6-9f5c-5abdc149f542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, tag, labels, ner_labels, label_len]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71758fb8-6aec-4750-960e-02eb93bb6d47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "      <th>labels</th>\n",
              "      <th>ner_labels</th>\n",
              "      <th>label_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71758fb8-6aec-4750-960e-02eb93bb6d47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71758fb8-6aec-4750-960e-02eb93bb6d47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71758fb8-6aec-4750-960e-02eb93bb6d47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_labels,hidden_size=768,dropout=0.1):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(hidden_size,num_labels)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:,0,:]\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        x = torch.softmax(x,dim=-1)\n",
        "        return x\n",
        "\n",
        "class BiLSTMCRF(nn.Module):\n",
        "  def __init__(self, num_classes,hidden_size=768,dropout=.1):\n",
        "      super(BiLSTMCRF, self).__init__()\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.bilstm = nn.LSTM(hidden_size, hidden_size // 2, dropout=dropout, batch_first=True, bidirectional=True)\n",
        "      self.linear = nn.Linear(hidden_size, num_classes)\n",
        "      self.crf = CRF(num_labels=num_classes)\n",
        "  def forward(self, features,masks=None, tags=None, using_pack_sequence=True):\n",
        "      last_encoder_layer = self.dropout(features)\n",
        "      if using_pack_sequence is True:\n",
        "          pack_padded_last_encoder_layer = pack_padded_sequence(last_encoder_layer, [512], batch_first=True, enforce_sorted=False)\n",
        "          outputs, hc = self.bilstm(pack_padded_last_encoder_layer)\n",
        "          outputs = pad_packed_sequence(outputs, batch_first=True, padding_value=0)[0]\n",
        "      else:\n",
        "          outputs, hc = self.bilstm(last_encoder_layer)\n",
        "      emissions = self.linear(outputs)\n",
        "      masks = torch.gt(masks,0)\n",
        "      if tags is not None: # crf training\n",
        "          log_likelihood, sequence_of_tags = self.crf(emissions, tags,masks), self.crf.viterbi_decode(emissions,mask=masks)\n",
        "          return log_likelihood, sequence_of_tags\n",
        "      else: # tag inference\n",
        "          sequence_of_tags = self.crf.viterbi_decode(emissions,masks)\n",
        "          return sequence_of_tags\n",
        "\n",
        "class ClassifierNER(nn.Module):\n",
        "    def __init__(self,bert,tokenizer,classifier,ner,clf_labels,ner_labels):\n",
        "        super(ClassifierNER,self).__init__()\n",
        "        self.bert = bert\n",
        "        self.ner = ner\n",
        "        self.classifier = classifier\n",
        "        self.tokenizer = tokenizer\n",
        "        self.clf_labels= clf_labels\n",
        "        self.ner_labels = ner_labels\n",
        "        \n",
        "    def forward(self,inputs,tags=None):\n",
        "        bert_outputs = self.bert(**inputs)\n",
        "        hidden_states = bert_outputs.last_hidden_state\n",
        "        clf_prediction = self.classifier(hidden_states)\n",
        "        ner_preds = self.ner(hidden_states,masks=inputs[\"attention_mask\"],tags=tags)\n",
        "        return {\"classification\":clf_prediction,\"entities\":ner_preds}   \n",
        "\n",
        "    def predict(self,input):\n",
        "        output = self.forward(input)\n",
        "        clf_prediction = torch.argmax(output['classification'])\n",
        "        clf_prediction = self.clf_labels[clf_prediction]\n",
        "        ner_prediction = output['entities'].tolist()\n",
        "        ner_prediction = [self.ner_labels[x] for x in ner_prediction]\n",
        "        return {\"classification\":clf_prediction,\"entities\":ner_prediction}"
      ],
      "metadata": {
        "id": "Bn27ncuwgIyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Classifier(2)\n",
        "ner = BiLSTMCRF(3)\n",
        "clf_ner = ClassifierNER(bert,tokenizer,clf,ner,{0:\"NEGATIVE\",1:\"POSITIVE\"},{0:\"B-gov_law\",1:\"I-gov_law\",2:\"O\"})"
      ],
      "metadata": {
        "id": "kOydfcYCgKJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_combined_dataloader(texts, labels, ner_labels, tokenizer, max_length, batch_size, train_size=0.8):\n",
        "    dataset = CombinedDataset(texts, labels, ner_labels, tokenizer, max_length)\n",
        "    train_size = int(train_size * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2,\n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    return train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "F-O8Jxdzg4sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,val_data = create_combined_dataloader(df['text'],df['labels'],df['ner_labels'],tokenizer,512,2)"
      ],
      "metadata": {
        "id": "B3PXJuwIgQlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "        for batch in tqdm(train_data):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'classification_labels' and k != 'ner_labels'}\n",
        "            clf_label = batch['classification_labels'].to(device)\n",
        "            ner_label = batch['ner_labels'].to(device)\n",
        "            output = model(inputs,ner_label)\n",
        "            clf_prediction = output['classification'].to(device)\n",
        "            ner_prediction = torch.tensor(output['entities'][0]).float().to(device)\n",
        "            clf_loss = F.cross_entropy(clf_prediction, clf_label)\n",
        "            ner_loss = -1 * ner_prediction.sum()\n",
        "            loss = clf_loss + ner_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            acc = (clf_prediction.argmax(dim=1) == clf_label).sum().item() / len(clf_label)\n",
        "            total_acc_train += acc\n",
        "            total_loss_train += loss.item()\n",
        "        avg_loss_train = total_loss_train / len(train_data)\n",
        "        avg_acc_train = total_acc_train / len(train_data)\n",
        "        print(f\"Train Loss: {avg_loss_train:.4f} Train Accuracy: {avg_acc_train:.4f}\")\n",
        "        \n",
        "        model.eval()\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "        for batch in tqdm(val_data):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'classification_labels' and k != 'ner_labels'}\n",
        "            clf_label = batch['classification_labels'].to(device)\n",
        "            ner_label = batch['ner_labels'].to(device)\n",
        "            output = model(inputs)\n",
        "            clf_prediction = output['classification']\n",
        "            ner_prediction = torch.tensor(output['entities'][0]).float().to(device)\n",
        "            clf_loss = F.cross_entropy(clf_prediction, clf_label)\n",
        "            ner_loss = -1 * ner_prediction.sum()\n",
        "            loss = clf_loss + ner_loss\n",
        "            acc = (clf_prediction.argmax(dim=1) == clf_label).sum().item() / len(clf_label)\n",
        "            total_acc_val += acc\n",
        "            total_loss_val += loss.item()\n",
        "        avg_loss_val = total_loss_val / len(val_data)\n",
        "        avg_acc_val = total_acc_val / len(val_data)\n",
        "        print(f\"Validation Loss: {avg_loss_val:.4f} Validation Accuracy: {avg_acc_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "OeZUVMcOgRm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(clf_ner,train_data,val_data,.01,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqED7O9rgSuR",
        "outputId": "6794e211-5b45-4b6d-ea97-e10ce2e8a039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/152 [00:00<?, ?it/s]<ipython-input-63-42388368bc3e>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  ner_prediction = torch.tensor(output['entities'][0]).float().to(device)\n",
            "100%|██████████| 152/152 [01:12<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 264.6371 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:04<00:00,  8.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3133 Validation Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 152/152 [01:21<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 264.6773 Train Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38/38 [00:04<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.3133 Validation Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['text'][0]\n",
        "inputs = tokenizer.encode_plus(text)\n",
        "clf_ner(**inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Ao-vhY6FvHMq",
        "outputId": "182325f5-3d21-4fac-8dce-16e84a6b2bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-1a946f1ed438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8L2V7_sx5C7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}